{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bryant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bryant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bryant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1757, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Ensure stopwords and other nltk resources are downloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "data_new = pd.read_csv('IKN-08.csv')\n",
    "\n",
    "# Remove duplicates and NaN/null values\n",
    "data_new.drop_duplicates(subset=['id_str'], keep='first', inplace=True)\n",
    "data_new.dropna(subset=['full_text', 'username'], inplace=True)\n",
    "\n",
    "# Keep only one data point per username\n",
    "data_new.drop_duplicates(subset=['username'], keep='first', inplace=True)\n",
    "\n",
    "# Text preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    # Case folding\n",
    "    text = text.lower()\n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\S+|#\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the full_text column\n",
    "data_new['processed_text'] = data_new['full_text'].apply(preprocess_text)\n",
    "\n",
    "# Sentiment analysis using VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data_new['Sentiment'] = data_new['processed_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Assign sentiment labels\n",
    "def sentiment_label(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data_new['Sentiment_Label'] = data_new['Sentiment'].apply(sentiment_label)\n",
    "\n",
    "data_new.head()\n",
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment_Label\n",
       "Neutral     1572\n",
       "Positive     136\n",
       "Negative      49\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_counts = data_new['Sentiment_Label'].value_counts()\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aetherienll</td>\n",
       "      <td>Wed May 15 23:54:40 +0000 2024</td>\n",
       "      <td>@tanyarlfes WKWKWKWK MIMPI APA LU PADA sekaran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kompasiana</td>\n",
       "      <td>Wed May 15 23:53:01 +0000 2024</td>\n",
       "      <td>Terjebak Macet selama Tiga Jam di Kawasan IKN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokastiti</td>\n",
       "      <td>Wed May 15 23:52:43 +0000 2024</td>\n",
       "      <td>Jadi presidennya mau. Giliran disuruh pindah k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kanoysim</td>\n",
       "      <td>Wed May 15 23:52:30 +0000 2024</td>\n",
       "      <td>Kebutuhan primer harus di utamakan bahkan di u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PartaiPonsel</td>\n",
       "      <td>Wed May 15 23:48:58 +0000 2024</td>\n",
       "      <td>@Andria75777 Mampus kelen...wkwkwkkw . Makan t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       username                      created_at  \\\n",
       "0   aetherienll  Wed May 15 23:54:40 +0000 2024   \n",
       "1    kompasiana  Wed May 15 23:53:01 +0000 2024   \n",
       "2     lokastiti  Wed May 15 23:52:43 +0000 2024   \n",
       "3      Kanoysim  Wed May 15 23:52:30 +0000 2024   \n",
       "5  PartaiPonsel  Wed May 15 23:48:58 +0000 2024   \n",
       "\n",
       "                                           full_text  \n",
       "0  @tanyarlfes WKWKWKWK MIMPI APA LU PADA sekaran...  \n",
       "1  Terjebak Macet selama Tiga Jam di Kawasan IKN ...  \n",
       "2  Jadi presidennya mau. Giliran disuruh pindah k...  \n",
       "3  Kebutuhan primer harus di utamakan bahkan di u...  \n",
       "5  @Andria75777 Mampus kelen...wkwkwkkw . Makan t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = data_new[['username', 'created_at', 'full_text']]\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('Final-IKN.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
